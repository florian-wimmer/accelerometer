{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from scipy.signal import iirfilter, filtfilt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import dill\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "\"\"\"\n",
    "Date: 2024-06-20 15:47:56,,,,,,\n",
    "Calibration: XL_ODR: 6667Hz, XL_FS: 16g, GY_ODR: 6667Hz, GY_FS: 2000dps,,,\n",
    "Time(s),Acceleration X (g),Acceleration Y (g),Acceleration Z (g),Angular Momentum X (dps),Angular Momentum Y (dps),Angular Momentum Z (dps)\n",
    "0.000083,0.693359,-0.720215,0.14209,-3.540039,0.366211,-3.051758\n",
    "0.00098,0.70752,-0.755371,0.086914,1.953125,0.732422,-1.159668\n",
    "\"\"\"\n",
    "\n",
    "# Check if GPU is available and use it if possible\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "#dill.load_session('nn_env.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data\n",
    "train_data = pd.read_csv(r\"../measurement/processed_data/train_long.csv\", header=2)\n",
    "#X_train = train_data[[\"Time(s)\", \"Acceleration X (g)\", \"Acceleration Y (g)\", \"Acceleration Z (g)\", \"Angular Momentum X (dps)\", \"Angular Momentum Y (dps)\", \"Angular Momentum Z (dps)\"]]\n",
    "X_train = train_data[[\"Time(s)\", \"Acceleration X (g)\", \"Acceleration Y (g)\", \"Angular Momentum Z (dps)\"]]\n",
    "y_train = train_data[\"Stand detected\"]\n",
    "\n",
    "sampling_frequency = 1 / (train_data[\"Time(s)\"].diff().mean())\n",
    "cutoff_frequency = 100\n",
    "\n",
    "# Apply IIR filter to training data (excluding time series)\n",
    "b, a = iirfilter(4, Wn=cutoff_frequency, fs=sampling_frequency, btype=\"low\", ftype=\"butter\")\n",
    "X_train_filtered = X_train.copy()\n",
    "X_train_filtered.iloc[:, 1:] = filtfilt(b, a, X_train.iloc[:, 1:], axis=0)\n",
    "#X_train_filtered = X_train.copy()\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_filtered.iloc[:, 1:])\n",
    "\n",
    "# Increase the weight of specific features\n",
    "#feature_weights = np.array([1, 1, 1, 1, 1, 1])  # Higher weights for Acc X, Acc Y, and Angular Z\n",
    "feature_weights = np.array([1, 1, 1])\n",
    "X_train_weighted = X_train_scaled * feature_weights\n",
    "\n",
    "# Convert data to float32\n",
    "X_train_weighted = X_train_weighted.astype(np.float32)\n",
    "\n",
    "# Convert data to PyTorch tensors and move to device\n",
    "X_train_tensor = torch.tensor(X_train_weighted).to(device)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).to(device)\n",
    "\n",
    "# Reintroduce TimeSeriesDataset\n",
    "class TimeSeriesDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, labels, seq_len=10, step=100):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.seq_len = seq_len\n",
    "        self.step = step\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - (self.seq_len - 1) * self.step\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        indices = [idx + i * self.step for i in range(self.seq_len)]\n",
    "        x_seq = self.data[indices]\n",
    "        y_val = self.labels[indices[-1]]\n",
    "        return x_seq, y_val\n",
    "    \n",
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(GRUModel, self).__init__()\n",
    "        # Bidirectional GRU with hidden size 64 (output dims = 128)\n",
    "        self.gru = nn.GRU(input_size, 64, num_layers=2, dropout=0.5, batch_first=True, bidirectional=True)\n",
    "        # Additional fully connected layers for post-processing\n",
    "        self.fc1 = nn.Linear(128, 32)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(32, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.gru(x)\n",
    "        # Take the output from the final time step\n",
    "        out = out[:, -1, :]  # shape: (batch, 128)\n",
    "        out = self.fc1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.sigmoid(out)\n",
    "        return out\n",
    "    \n",
    "sequence_length = 10\n",
    "step_size = 100\n",
    "batch_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TimeSeriesDataset(X_train_tensor, y_train_tensor, sequence_length, step_size)\n",
    "dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "model = GRUModel(input_size=X_train_weighted.shape[1]).to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    with tqdm(dataloader, unit=\"batch\") as tepoch:\n",
    "        for batch_X, batch_y in tepoch:\n",
    "            tepoch.set_description(f\"Epoch {epoch+1}\")\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs.squeeze(), batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            tepoch.set_postfix(loss=loss.item())\n",
    "\n",
    "torch.save(model.state_dict(), 'model/gru_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GRUModel(input_size=X_train_weighted.shape[1]).to(device)\n",
    "model.load_state_dict(torch.load('model/gru_model.pth'))\n",
    "\n",
    "# Load test data\n",
    "test_data = pd.read_csv(r\"../measurement/processed_data/moving_all_around/backward.csv\", header=2)\n",
    "X_test = test_data[[\"Time(s)\", \"Acceleration X (g)\", \"Acceleration Y (g)\", \"Angular Momentum Z (dps)\"]]\n",
    "\n",
    "# Apply IIR filter to test data (excluding time series)\n",
    "X_test_filtered = X_test.copy()\n",
    "X_test_filtered.iloc[:, 1:] = filtfilt(b, a, X_test.iloc[:, 1:], axis=0)\n",
    "\n",
    "# Standardize the test data\n",
    "X_test_scaled = scaler.transform(X_test_filtered.iloc[:, 1:])\n",
    "\n",
    "# Convert test data to PyTorch tensor\n",
    "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32).to(device)\n",
    "\n",
    "# Prepare test dataset with sequence windows and change to batch prediction\n",
    "test_dataset = TimeSeriesDataset(X_test_tensor, torch.zeros(len(X_test_tensor)), sequence_length)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "all_preds = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch_X, _ in test_loader:\n",
    "        batch_X = batch_X  # shape: (batch, seq_len, features)\n",
    "        batch_preds = (model(batch_X).squeeze() > 0.5)\n",
    "        all_preds.extend(batch_preds.tolist())\n",
    "\n",
    "# Align predictions with the original test indices.\n",
    "\n",
    "pred_array = np.full(len(X_test_tensor), np.nan)\n",
    "for i, pred in enumerate(all_preds):\n",
    "    index = i + (sequence_length - 1) * step_size \n",
    "    if index < len(pred_array):\n",
    "        pred_array[index] = pred\n",
    "\n",
    "test_data[\"predicted_heel_button\"] = pred_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Angular Momentum Z axis (filtered and scaled)\n",
    "fig = go.Figure()\n",
    "# [:, 5]\n",
    "fig.add_trace(go.Scatter(x=test_data[\"Time(s)\"], y=X_test_scaled[:, 0], mode=\"lines\", name=\"AC X (Filtered & Scaled)\"))\n",
    "fig.add_trace(go.Scatter(x=test_data[\"Time(s)\"], y=X_test_scaled[:, 1], mode=\"lines\", name=\"AC Y (Filtered & Scaled)\"))\n",
    "fig.add_trace(go.Scatter(x=test_data[\"Time(s)\"], y=X_test_scaled[:, 2], mode=\"lines\", name=\"AM Z (Filtered & Scaled)\"))\n",
    "fig.add_trace(go.Scatter(x=test_data[\"Time(s)\"], y=test_data[\"Stand detected\"], mode=\"lines\", name=\"Heel Button\"))\n",
    "#fig.add_trace(go.Scatter(x=test_data[\"Time(s)\"], y=test_data[\"Stand detected\"], mode=\"lines\", name=\"Analytical\"))\n",
    "fig.add_trace(go.Scatter(x=test_data[\"Time(s)\"], y=test_data[\"predicted_heel_button\"], mode=\"lines\", name=\"Predicted Heel Button\"))\n",
    "\n",
    "equal_values = (test_data[\"predicted_heel_button\"] == test_data[\"Stand detected\"]).sum()\n",
    "\n",
    "print(f\"{equal_values} out of {test_data.shape[0]} entries are equal, which is {equal_values / test_data.shape[0] * 100:.2f}%\")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dill.dump_session('gru_env.db')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
